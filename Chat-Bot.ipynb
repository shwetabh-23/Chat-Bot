{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from IPython.display import clear_output\n",
    "import random\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://support.microsoft.com/en-us/word'\n",
    "headers={'User-Agent':'Mozilla/5.0 (Windows NT 6.3; Win 64 ; x64) Apple WeKit /537.36(KHTML , like Gecko) Chrome/80.0.3987.162 Safari/537.36'}\n",
    "webpage = requests.get(url, headers=headers).text\n",
    "soup = BeautifulSoup(webpage, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_urls_typ1(url):\n",
    "        get_urls_l = []\n",
    "        webpage = requests.get(url, headers = headers).text\n",
    "        soup = BeautifulSoup(webpage, 'html.parser')\n",
    "        parent_i = soup.find_all('li', class_ = 'supMultimediaLeftNavArticle')\n",
    "        for i in range(int(len(parent_i)/2)):\n",
    "            parent_f = parent_i[i].find('a')\n",
    "            get_urls_l.append('https://support.microsoft.com' + parent_f.attrs['href']) \n",
    "        return get_urls_l  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_urls_typ2(url):\n",
    "        get_urls_l = []\n",
    "        webpage = requests.get(url, headers = headers).text\n",
    "        soup = BeautifulSoup(webpage, 'html.parser')\n",
    "        parent_i = soup.find_all('p', class_ = 'ocpAlertSection')\n",
    "        for i in range(int(len(parent_i))):\n",
    "            parent_f = parent_i[i].find('a')\n",
    "            get_urls_l.append('https://support.microsoft.com' + parent_f.attrs['href']) \n",
    "        return get_urls_l  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_urls_typ3(url):\n",
    "        get_urls_l = []\n",
    "        webpage = requests.get(url, headers = headers).text\n",
    "        soup = BeautifulSoup(webpage, 'html.parser')\n",
    "        parent_i = soup.find_all('a', class_ = 'ocpArticleLink')\n",
    "        for i in range(len(parent_i)):\n",
    "           # print(parent_i[i].attrs['href'])\n",
    "            get_urls_l.append('https://support.microsoft.com' + parent_i[i].attrs['href']) \n",
    "        return get_urls_l  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_urls(url):\n",
    "    urls = []\n",
    "    new_urls_l = []\n",
    "    new = []\n",
    "    webpage = requests.get(url, headers = headers).text\n",
    "    soup = BeautifulSoup(webpage, 'html.parser')\n",
    "    parent_i = soup.find_all('li', class_ = 'nav-gallery__cta-grid__column--glyph')\n",
    "    for i in range(len(parent_i)):\n",
    "        parent_f = parent_i[i].find('a')\n",
    "        urls.append('https://support.microsoft.com' + parent_f.attrs['href'])\n",
    "    len_of_url_f = len(urls)    \n",
    "    len_of_url_i = 0\n",
    "    for url in urls[len_of_url_i:len_of_url_f]:    \n",
    "        try:        \n",
    "            new_urls = get_urls_typ1(url)\n",
    "            for i in new_urls[1:]:\n",
    "                urls.append(i)\n",
    "        except :\n",
    "            pass\n",
    "        try:\n",
    "            new_urls = get_urls_typ2(url)\n",
    "            for i in new_urls:\n",
    "                urls.append(i)\n",
    "        except:\n",
    "            pass\n",
    "        else:\n",
    "            new_urls = get_urls_typ3(url)\n",
    "            for i in new_urls:\n",
    "                urls.append(i)  \n",
    "        len_of_url_i = len_of_url_f\n",
    "        len_of_url_f = len(urls)\n",
    "    return (set(urls))     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = list(find_urls(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(url):\n",
    "    heading = []\n",
    "    para = []\n",
    "\n",
    "    webpage = requests.get(url, headers=headers).text\n",
    "    soup = BeautifulSoup(webpage, 'html.parser')\n",
    "    try:\n",
    "        parent = soup.find_all('div', class_ = 'supARG-column-1-2')\n",
    "        for i in range(len(parent)):\n",
    "            parent_i = parent[i].find_all('p')\n",
    "            for j in range(1, len(parent_i)):\n",
    "                heading.append(parent_i[0].text.strip())\n",
    "                para.append(parent_i[j].text.strip())\n",
    "        parent_head = soup.find_all('h1')\n",
    "        for i in range(len(parent_head)):\n",
    "            heading.append(parent_head[i].text.strip())          \n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        parent = soup.find_all('div', class_ = 'supTabControlTabContent')\n",
    "        \n",
    "        parent_head = soup.find_all('h1')\n",
    "        for i in range(len(parent_head)):\n",
    "            heading.append(parent_head[i].text.strip())\n",
    "\n",
    "        for i in range(len(parent)):\n",
    "            parent_i = parent[i].find_all('p')\n",
    "            for j in range(len(parent_i)):\n",
    "                para.append(((parent_i[j].text.strip())))\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        parent = soup.find_all('article', class_ = 'ocpArticleContent')\n",
    "        parent_f = soup.find_all('h1')\n",
    "        for i in range(len(parent_f)):\n",
    "            heading.append(parent_f[i].text.strip())\n",
    "        for i in range(len(parent)):\n",
    "            parent_i = parent[i].find_all('p')\n",
    "            for j in range(len(parent_i)):\n",
    "                para.append(parent_i[j].text.strip())\n",
    "    except:\n",
    "        pass\n",
    "    return list(reversed(list(set(heading)))), list(reversed(list(set(para)))) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "intents = {}\n",
    "for i in range(len(urls)):\n",
    "    try:\n",
    "        head, para = extract_text(urls[i])\n",
    "       # print(head, para)\n",
    "        for heading in head:\n",
    "            intents[heading] = para\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(text):\n",
    "    y = []\n",
    "    for i in text.split():\n",
    "        y.append(ps.stem(i))\n",
    "    return ' '.join(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(intents.items())\n",
    "df.columns = ['Headings', 'details']\n",
    "for column in df.columns[1:]:\n",
    "    df[column] = df[column].apply(lambda x : ' '.join(x))\n",
    "for column in df.columns:\n",
    "    df[column] = df[column].apply(lambda x : x.lower())   \n",
    "for column in df.columns:\n",
    "    df[column] = df[column].apply(lambda x : stemming(x))   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Headings sent tokenize'] = df['Headings'].apply(lambda x : sent_tokenize(x))\n",
    "df['details word tokenize'] = df['details'].apply(lambda x : word_tokenize(x))\n",
    "df['Headings word tokenize'] = df['Headings'].apply(lambda x : word_tokenize(x))\n",
    "df['details sent tokenize'] = df['details'].apply(lambda x : sent_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sent = []\n",
    "for i in range(len(df)):\n",
    "    all_sent.append(df['Headings'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "remove_pinct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_pinct_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "greeting_input = ('hello', 'hi', 'greetings', 'sup', 'whats up', 'hey')\n",
    "greeting_responses = ('hi', 'hey', '*nods*', 'hi there', 'hello', 'I am glad')\n",
    "\n",
    "def greet(sent):\n",
    "    for word in sent.split():\n",
    "        if word.lower() in greeting_input:\n",
    "            return random.choice(greeting_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot_response(chatbot_input):\n",
    "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
    "    tfidf = TfidfVec.fit_transform(all_sent)\n",
    "    tfidf_new = TfidfVec.transform(sent_tokenize(chatbot_input))\n",
    "    vals = cosine_similarity(tfidf_new, tfidf)\n",
    "    idx = vals.argsort()[0][-1]\n",
    "    flat = vals.flatten()\n",
    "    flat.sort()\n",
    "    req_tfidf = flat[-1]\n",
    "    if req_tfidf == 0 :\n",
    "        return 'Sorry I do not understand this'\n",
    "    \n",
    "    else:\n",
    "        for i in range(len(df['details sent tokenize'][idx])):\n",
    "            print(df['details sent tokenize'][idx][i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOT : \n",
      "Sorry I do not understand this\n",
      "i am satisfied with my care\n",
      "BOT : Shutting Down\n"
     ]
    }
   ],
   "source": [
    "flag = True\n",
    "print('BOT : Hello!! I am Baymax, Your personal MS Word Care Assistant. What can I help you with? When you are done, Please say bye or thank you')\n",
    "while(flag == True):\n",
    "    chatbot_input = input()\n",
    "    chatbot_input = chatbot_input.lower()    \n",
    "    \n",
    "    if (chatbot_input == 'i am satisfied with my care'):\n",
    "        flag = False\n",
    "        print('BOT : Shutting Down')\n",
    "        \n",
    "    elif (chatbot_input == 'bye' or chatbot_input == 'thats all' or chatbot_input == 'thanks' or chatbot_input == 'thank you' ):\n",
    "        print('BOT : OKAY, but ....I cannot deactivate until you say, i am satisfied with my care')\n",
    "    \n",
    "    elif (chatbot_input == 'i am satisfied with my care'):\n",
    "        flag = False\n",
    "        print('BOT : Shutting Down')\n",
    "        \n",
    "    elif chatbot_input in greeting_input :\n",
    "        print('BOT : ' + greet(chatbot_input))    \n",
    "    \n",
    "    else:\n",
    "        chatbot_input = stemming(chatbot_input)\n",
    "        \n",
    "        clear_output(wait = False)\n",
    "        print( 'BOT : ')\n",
    "        print(chatbot_response(chatbot_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
